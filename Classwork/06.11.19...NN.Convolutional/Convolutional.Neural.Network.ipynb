{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤” Convolutional Neural Network ðŸ¤”\n",
    "\n",
    "This is one way....\n",
    "\n",
    "* It's possible to apply MLP (multi-layer perceptron) to image data\n",
    "* Dataset we use for image is MNIST (handwritten digits)\n",
    "* Dimensions of each image is 28x28\n",
    "    * Can be repped by a **matrix** (2D tensor)\n",
    "    * To use MLP, we **reshape** the image data into a single 1D tensor (1 column, **784** rows from 28x28)\n",
    "    * And this is the input layer\n",
    "* Loss we should choose is **categorical cross entropy**\n",
    "\n",
    "<font size=\"+1\">Multi-class --> softmax --> categorical cross-entropy</font>\n",
    "\n",
    "<font size=\"+1\">Multi-class --> multi-label --> sigmoid</font>\n",
    "\n",
    "...AND NOW WE'RE GONNA FIND ANOTHER WAY! It mimics the human visual perception and understanding and translation of cognition\n",
    "\n",
    "Enter **convolutional neural network**, which is architecturally *completely different* from MLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  CNN\n",
    "\n",
    "* **Super flipping good for images**\n",
    "* CNN is basically 2D configuration of NN\n",
    "* Input: **image** (3 NxN if *color*, NxN if *black-and-white*)\n",
    "* Weights: **2D array**\n",
    "\n",
    "<img src=\"rgb_image.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights in CNN\n",
    "\n",
    "* called **kernel** or **filter matrix**\n",
    "* element-wise multiplication and addition to get the result\n",
    "* You can have many kernels, which creates a **cube** of quasi-stacked matrices. This is a **hyperparameter** (and it *is possible to overfit*)\n",
    "\n",
    "<img src=\"kernel_image.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "* the horizontal and vertical movement steps\n",
    "* What is the output size with stride = 1 and stride = 2?\n",
    "* ```output_size = (input_size - filter_size)/stride + 1```\n",
    "    * Stride = 1: (7-3)/1 + 1 = 5\n",
    "    * Stride = 2: (7-3)/2 + 1 = 3\n",
    "\n",
    "<img src=\"stride_1.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "<img src=\"stride_2.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "* Stride visualization: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=input_shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling vs Average Pooling\n",
    "\n",
    "* Max pooling: take the **maxiumum** element from each window of a certain size\n",
    "\n",
    "<img src=\"maxpooling.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "\n",
    "* After feature extraction is completed by multiple *convolutional layers*, we use **flatten layer** to add MLP (*after convolutional layers*) to **classify**\n",
    "* ```Keras```'s version of ```numpy.reshape```. Simple!\n",
    "* Reshapes *n*-dimensional arrays to a vector -- necessary when moving from Conv2D layers, which expect 2-D arrays as inputs\n",
    "    * Concrete example: A Flatten layer given a 28 x 28 array as input would output a vector of the shape (784, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the whole NN: CNN (feature learning) + MLP (classification)\n",
    "\n",
    "<img src=\"CNN.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’š Activity: Obtain the number of parameters for the following CNN\n",
    "\n",
    "* By default, the strides = (1, 1)\n",
    "* ```model.summary()``` tells us the **output shape** of our layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/cherishkim/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "* 64 x 32 x 3 x 3 + 64 = **18496**\n",
    "* 12 x 12 x 64 = **9216**\n",
    "* 9216 x 128 + 128 = **1179776**\n",
    "* 128 x 10 + 10 = **1290**\n",
    "* ADD THEM ALL! = **1199882** --> That's the total # of parameters!\n",
    "\n",
    "<img src=\"diagram.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
