{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)\n",
    "\n",
    "- The idea behind RNNs is to make use of sequential information\n",
    "\n",
    "- In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea\n",
    "\n",
    "- If you want to predict the next word in a sentence you better know which words came before it\n",
    "\n",
    "- Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far\n",
    "\n",
    "<img src=\"simple_rnn.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Cells in Keras\n",
    "\n",
    "- SimpleRNN\n",
    "\n",
    "- LSTM \n",
    "\n",
    "- GRU\n",
    "\n",
    "<img src=\"LSTM.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The time steps defines how many times the LSTM cell state is updated for one sample (one mnist digit for example)\n",
    "\n",
    "- To use LSTM for image classification we should prepare our data such that it has sequential meaning\n",
    "\n",
    "- Lets pepared data (image here) for Sequential MNIST Classification\n",
    "\n",
    "- We use 28 sequence (time step) each with 28 features\n",
    "\n",
    "<img src=\"mnist_lstm.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Train a LSTM model with Keras for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_test)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# print(x_train[1])\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "print(x_train[0])\n",
    "nb_units = 50\n",
    "\n",
    "model = Sequential()\n",
    "# input_shape for LSTM shoud be (time steps, features)\n",
    "model.add(LSTM(nb_units, input_shape=(28, 28)))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "# 2.5 Compile the model.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 2.6 Print out model.summary\n",
    "epochs = 3\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=128,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Sequence in LSTM\n",
    "\n",
    "- `model.add(LSTM(nb_units, input_shape=(28, 28), return_sequences = False))`\n",
    "\n",
    "<img src=\"return_seq_F.png\" width=\"250\" height=\"250\">\n",
    "\n",
    "\n",
    "- `model.add(LSTM(nb_units, input_shape=(28, 28), return_sequences = True))`\n",
    "\n",
    "<img src=\"return_seq_T.png\" width=\"250\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the LSTM model for MNIST look like?\n",
    "\n",
    "<img src=\"mnist_lstm_nn.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many parameters LSTM has?\n",
    "\n",
    "- Assume the subscript *t* indexes the time step\n",
    "\n",
    "<img src=\"lstm_math.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "- We have four W and four U and four bias\n",
    "\n",
    "- The number of parameters for LSTM is 4dh + 4 hh  + 4h. The last term is for four bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Verify the number of parameters for LSTM in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "input_array = np.array([[[0], [1], [2], [3], [4]], [[5], [1], [2], [3], [6]]])\n",
    "print(input_array.shape)\n",
    "model = Sequential()\n",
    "# input_shape for LSTM shoud be (time steps, features)\n",
    "model.add(LSTM(10, input_shape=(5, 1), return_sequences=False))\n",
    "model.summary()\n",
    "print(input_array)\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "# the number of parameters of a LSTM layer in Keras equals to\n",
    "# params = 4 * ((size_of_input + 1) * size_of_output + size_of_output^2)\n",
    "n_params = 4 * ((1 + 1) * 10 + 10**2)\n",
    "print(n_params)\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
